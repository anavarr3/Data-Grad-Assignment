summarise(total = sum(amount))
umd_payments <- cleaned_payments |>
filter(
str_detect(agency_name, "CAPITAL PROJECTS- UNIVERSITY OF MARYLAND")) |>
group_by(vendor_name, vendor_zip) |>
summarise(total = sum(amount))
umd_payments
umd_payments <- cleaned_payments |>
filter(
str_detect(agency_name, "CAPITAL PROJECTS- UNIVERSITY OF MARYLAND")) |>
group_by(vendor_name, vendor_zip) |>
summarise(total = sum(amount))
arrange(desc(total))
umd_payments <- cleaned_payments |>
filter(
str_detect(agency_name, "CAPITAL PROJECTS- UNIVERSITY OF MARYLAND")) |>
group_by(vendor_name, vendor_zip) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
umd_payments
View(cleaned_md_grants_loans)
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT")) |>
group_by(ficsal_year, grantee_name, amount) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT")) |>
group_by(ficsal_year) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT")) |>
group_by(fiscal_year, grantee_name, amount) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT")) |>
group_by(fiscal_year, grantee, amount) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
cleaned_md_grants_loans
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT")) |>
group_by(fiscal_year, grantee, amount) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
cleaned_md_grants_loans
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT")) |>
# group_by(fiscal_year, grantee, amount) |>
# summarise(total = sum(amount)) |>
#arrange(desc(total))
cleaned_md_grants_loans
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT"))
# group_by(fiscal_year, grantee, amount) |>
# summarise(total = sum(amount)) |>
#arrange(desc(total))
cleaned_md_grants_loans
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant,STEM CELL RESEARCH GRANT"))
# group_by(fiscal_year, grantee, amount) |>
# summarise(total = sum(amount)) |>
#arrange(desc(total))
cleaned_md_grants_loans
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT"))
# group_by(fiscal_year, grantee, amount) |>
# summarise(total = sum(amount)) |>
#arrange(desc(total))
cleaned_md_grants_loans
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT"))
# group_by(fiscal_year, grantee, amount) |>
# summarise(total = sum(amount)) |>
#arrange(desc(total))
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT"))
group_by(fiscal_year, grantee, amount) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT"))
group_by(fiscal_year) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT")) |>
group_by(fiscal_year, grantee, amount) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT"),
str_detect(grantor, "Maryland|MD|MARYLAND")) |>
group_by(fiscal_year, grantee, amount) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT"),
str_detect(grantor, "Maryland|MD|MARYLAND")) |>
group_by(fiscal_year, grantor, grantee, amount) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT"),
str_detect(grantor, "Maryland|MD|MARYLAND|state|STATE")) |>
group_by(fiscal_year, grantor, grantee, amount) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT"),
str_detect(grantor, "Maryland|MD|MARYLAND|state|STATE|department|DEPARTMENT")) |>
group_by(fiscal_year, grantor, grantee, amount) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT"),
str_detect(grantor, "Maryland|MD|MARYLAND|state|STATE|department|DEPARTMENT")) |>
group_by(fiscal_year) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
cleaned_md_grants_loans |>
filter(
str_detect(description, "Stem Cell Research Grant|STEM CELL RESEARCH GRANT"),
str_detect(grantor, "Maryland|MD|MARYLAND|state|STATE|department|DEPARTMENT")) |>
group_by(fiscal_year) |>
summarise(total = sum(amount)) |>
arrange(desc(fiscal_year))
cleaned_md_grants_loans |>
filter(
str_detect(zip_code, "20742|20742-0001")), |>
cleaned_md_grants_loans |>
filter(
str_detect(zip_code, "20742|20742-0001")) |>
group_by(grantees)
cleaned_md_grants_loans |>
filter(
str_detect(zip_code, "20742|20742-0001")) |>
group_by(grantee)
cleaned_md_grants_loans |>
filter(
str_detect(zip_code, "20742|20742-0001|207420001")) |>
group_by(grantee)
cleaned_md_grants_loans |>
filter(
str_detect(zip_code, "20742|20742-0001|207420001|207423141")) |>
group_by(grantee)
cleaned_md_grants_loans |>
filter(
str_detect(zip_code, "20742|20742-0001|207420001|207423141")) |>
group_by(grantee) |>
summarise(total = sum(amount)) |>
arrange(desc(fiscal_year))
cleaned_md_grants_loans |>
filter(
str_detect(zip_code, "20742|20742-0001|207420001|207423141")) |>
group_by(grantee, amount, fiscal_year) |>
summarise(total = sum(amount)) |>
arrange(desc(fiscal_year))
cleaned_md_grants_loans |>
filter(
str_detect(zip_code, "20742|20742-0001|207420001|207423141")) |>
group_by(grantee, fiscal_year) |>
summarise(total = sum(amount)) |>
arrange(desc(fiscal_year))
cleaned_md_grants_loans |>
filter(
str_detect(zip_code, "20742|20742-0001|207420001|207423141")) |>
group_by(grantee) |>
summarise(total = sum(amount)) |>
arrange(desc(fiscal_year))
cleaned_md_grants_loans |>
filter(
str_detect(zip_code, "20742|20742-0001|207420001|207423141")) |>
group_by(grantee, fiscal_year) |>
summarise(total = sum(amount)) |>
arrange(desc(fiscal_year))
cleaned_md_grants_loans |>
filter(
str_detect(zip_code, "20742|20742-0001|207420001|207423141")) |>
group_by(grantee, amount) |>
summarise(total = sum(amount)) |>
arrange(desc(fiscal_year))
cleaned_md_grants_loans |>
filter(
str_detect(zip_code, "20742|20742-0001|207420001|207423141")) |>
group_by(grantee) |>
summarise(total = sum(amount)) |>
arrange(desc(total))
knitr::opts_chunk$set(echo = TRUE)
# Put code to reverse engineer sentence here
#Who are the people with no ID?
no_id <- clean_dc |>
filter(is.na(id))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(janitor)
library(readr)
library(refinr)
# Load required data
# Path to data should be loaded from folder "data" i.e. read_csv("data/name_of_data.csv")
dc_data <- read_csv("data/dc-wikia-data.csv")
marvel_data <- read_csv("data/marvel-wikia-data.csv")
dc_womenincomics2014_comparedto2013 <- read_csv("data/DC 2014 Women In Comics Stats.csv")
marvel_womenincomics2014_comparedto2013 <- read_csv("data/MARVEL 2014 Women In Comics Stats.csv")
dc_womenincomics2014_month <- read_csv("data/MONTH DC Women In Comics 2014.csv")
marvel_womenincomics2014_month <- read_csv("data/MONTH MARVEL Women In Comics 2014.csv")
# Clean required data and prepare for analysis if needed.
clean_dc <- clean_names(dc_data)
clean_marvel <- clean_names(marvel_data)
# Put code to reverse engineer sentence here
#DC code
dc_gender <- clean_dc |>
filter(!is.na(sex), appearances >= 1) |>
group_by(sex) |>
summarise(
count=n()) |>
arrange(desc(count)) |>
mutate(percentage = (count/sum(count)*100))
#Marvel code
marvel_gender <- clean_marvel |>
filter(!is.na(sex), appearances >= 1) |>
group_by(sex) |>
summarise(
count=n()) |>
arrange(desc(count)) |>
mutate(percentage = (count/sum(count)*100))
# Put code to reverse engineer sentence here
#DC code
dc_LGBT <- clean_dc |>
filter(!is.na(gsm))
dc_girls_gays_and_theys <- dc_LGBT |>
group_by(year) |>
summarise(
count_year=n()
)
# Marvel code
marvel_LGBT <- clean_marvel |>
filter(!is.na(gsm))
marvel_girls_gays_and_theys <- marvel_LGBT |>
group_by(year) |>
summarise(
count_year=n()
)
# Put code to reverse engineer sentence here
#Who are the people with no ID?
no_id <- clean_dc |>
filter(is.na(id))
no_id
# They are more side characters than heroes. We can likely exclude them from the code to get to the author's main findings
# Here we're looking for the secret identity split up in DC Comics
dc_secret_identity <- clean_dc |>
filter(!is.na(sex),
(!is.na(id))) |>
group_by(sex, id) |>
summarise(count=n()) |>
mutate(percentage = (count/sum(count)*100)) |>
arrange(desc(count))
# Now for Marvel
marvel_secret_identity <- clean_marvel |>
filter(!is.na(id),
(!is.na(id))) |>
group_by(sex, id) |>
summarise(count=n()) |>
mutate(percentage = (count/sum(count)*100)) |>
arrange(desc(count))
clean_marvel |>
group_by(id) |>
summarise(count = n())
#count it w/o known to authorities, with public, with secret identity
marvel_secret_identity_without_known_authorities <- clean_marvel |>
filter(!is.na(sex),
!is.na(id),
id != "Known To Authorities Identity") |>  # Exclude rows with the title "known to authorities"
group_by(sex, id) |>
summarise(count=n()) |>
mutate(percentage = (count/sum(count)*100)) |>
arrange(desc(count))
view(marvel_secret_identity_without_known_authorities)
marvel_secret_identity_without_known_authorities <- clean_marvel |>
filter(!is.na(sex),
!is.na(id),
id != "Known To Authorities Identity") |>  # Exclude rows with the title "known to authorities"
group_by(sex, id) |>
summarise(count=n()) |>
mutate(percentage = (count/sum(count)*100)) |>
arrange(desc(count))
view(marvel_secret_identity_without_known_authorities)
marvel_secret_identity_without_known_authorities <- clean_marvel |>
filter(!is.na(sex),
!is.na(id),
id != "Known To Authorities Identity") |>  # Exclude rows with the title "known to authorities"
group_by(sex, id) |>
summarise(count=n()) |>
mutate(percentage = (count/sum(count)*100)) |>
arrange(desc(count))
view(marvel_secret_identity_without_known_authorities)
marvel_secret_identity_without_known_authorities <- clean_marvel |>
filter(!is.na(sex),
!is.na(id),
id = !"Known To Authorities Identity") |>  # Exclude rows with the title "known to authorities"
group_by(sex, id) |>
summarise(count=n()) |>
mutate(percentage = (count/sum(count)*100)) |>
arrange(desc(count))
marvel_secret_identity_without_known_authorities <- clean_marvel |>
filter(!is.na(sex),
!is.na(id),
id != "Known To Authorities Identity") |>  # Exclude rows with the title "known to authorities"
group_by(sex, id) |>
summarise(count=n()) |>
mutate(percentage = (count/sum(count)*100)) |>
arrange(desc(count))
view(marvel_secret_identity_without_known_authorities)
marvel_secret_identity_without_known_authorities <- clean_marvel |>
filter(!is.na(sex),
!is.na(id),
id != "Known To Authorities Identity",
id != "No Dual Identity") |>  # Exclude rows with the title "known to authorities"
group_by(sex, id) |>
summarise(count=n()) |>
mutate(percentage = (count/sum(count)*100)) |>
arrange(desc(count))
view(marvel_secret_identity_without_known_authorities)
marvel_secret_identity_without_known_authorities <- clean_marvel |>
filter(!is.na(sex),
!is.na(id),
id != "Known To Authorities Identity") |>  # Exclude rows with the title known to authorities"
group_by(sex, id) |>
summarise(count=n()) |>
mutate(percentage = (count/sum(count)*100)) |>
arrange(desc(count))
view(marvel_secret_identity_without_known_authorities)
knitr::opts_chunk$set(echo = TRUE)
acs_income <- get_acs(geography="zcta", variables = "B19013_001", year=2021)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(lubridate)
library(janitor)
library(tidycensus)
library(readr)
library(stringr)
library("gganimate")
library("plotly")
PG911_OD_calls <- read_csv("data/prince_georges_2022_overdoses.csv") |>
clean_names() |>
separate(datetime, c("date", "time"), sep = " ") |>
mutate(date = ymd(date)) |>
mutate(month = month(date, label = TRUE)) |>
mutate(year = year(date)) |>
mutate(week = week(date))
PG911_OD_calls
PG911_OD_calls <- read_csv("data/prince_georges_2022_overdoses.csv") |>
clean_names() |>
separate(datetime, c("date", "time"), sep = " ") |>
mutate(date = ymd(date)) |>
mutate(month = month(date, label = TRUE)) |>
mutate(year = year(date)) |>
mutate(week = week(date))
PG911_OD_calls
view(PG911_OD_calls)
PG_OD_calls_by_month <- PG911_OD_calls |>
group_by(month) |>
summarise(total_calls = n()) |>
mutate(percentage_of_calls = total_calls/sum(total_calls)*100) |>
arrange(desc(percentage_of_calls))
PG_OD_calls_by_month
PG_OD_calls_by_month |>
ggplot() +
geom_bar(aes(x=reorder(month,percentage_of_calls), weight=percentage_of_calls), fill="grey", color="grey") +
coord_flip() +
theme(
plot.title = element_text(margin = margin(b = 5))
) +
labs(
title = "December saw highest percentage of overdose calls",
subtitle = "Most overdose calls in 2023 occurred in December for Prince George's County",
x = "month",
y = "percentage of calls",
caption = "source: Prince George's County EMS")
calls_by_week <- PG911_OD_calls |>
filter(!(week == 53)) |>
group_by(week) |>
summarise(total_calls = n()) |>
arrange(week)
calls_by_week
calls_by_week |>
ggplot() +
geom_line(aes(x=week, y=total_calls)) +
scale_x_continuous(breaks = 1:52) +
labs(
title="911 overdose calls spiked drastically in July 2023",
x = "month",
y = "total calls",
caption = "source: Prince George's County EMS") +
theme(
axis.text.x = element_text(angle = 45,  hjust=1)
)
knitr::include_graphics("https://www.mathsisfun.com/data/images/scatter-ice-cream1.svg")
PG_zip <- PG911_OD_calls |>
group_by(zipcode) |>
summarise(total_calls = n()) |>
arrange(desc(total_calls))
PG_zip
acs_income <- get_acs(geography="zcta", variables = "B19013_001", year=2021)
acs_income
PG_zip_merge <- PG_zip |>
left_join(acs_income, by = c('zipcode' = 'GEOID')) |>
rename(median_income = estimate) |>
filter(!(zipcode == "Q"),
!(zipcode == 0),
!(is.na(zipcode)),
!(is.na(median_income))) |>
select(zipcode, total_calls, median_income, moe)
PG_zip_merge
#There was a mess of parenthesis in the original code and I couldn't for the life of me see where I was missing some. I had to throw the code into ChatGPT to catch them. Wanted to clean the data since the scatterplot was having a time with the zip codes with no data or weird entries, like a "Q" zip code.
ggplot(PG_zip_merge, aes(x=median_income, y=total_calls)) +
geom_point()+
labs(
title="Overdose calls higher in zip codes with median household income of 75k and lower",
subtitle= "The number of calls in PG County throughout 2023 were concentrated in lower income households",
x = "median household income",
y = "total calls",
caption = "source: Prince George's County EMS, U.S. Census") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
geom_text(check_overlap = TRUE, size = 2, aes(label = zipcode), hjust = 0.5, vjust = -0.5)
setwd("~/Documents/GitHub/Data-Grad-Assignment")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(janitor)
read_csv("data/compliance") |>
clean_names()
read_csv("data/compliance.csv") |>
clean_names()
read_csv("data/compliance.csv") |>
clean_names() |>
filter!(is.na(document))
read_csv("data/compliance.csv") |>
clean_names() |>
filter(is.na!(document))
read_csv("data/compliance.csv") |>
clean_names() |>
filter(!is.na(document))
read_csv("data/compliance.csv") |>
clean_names() |>
#  filter(!is.na(document))
#Here I checked to see if I should keep the document column in. It seems like these links are invalid, so I'm going to select them out for now.
select(!document)
read_csv("data/compliance.csv") |>
clean_names() |>
#  filter(!is.na(document))
#Here I checked to see if I should keep the document column in. It seems like these links are invalid, so I'm going to select them out for now.
select(!document) |>
group_by(inspection_reason) |>
summarise(count=n())
read_csv("data/compliance.csv") |>
clean_names() |>
#  filter(!is.na(document))
#^ Here I checked to see if I should keep the document column in. It seems like these links are invalid, so I'm going to select them out for now.
select(!document, !inspection_reason)
# group_by(inspection_reason) |>
# summarise(count=n())
#^ Here I wanted to see if there was any reason to keep the "inspection_reason" column. Unfortunately, it's all NA, so I took it out.
read_csv("data/compliance.csv") |>
clean_names() |>
#  filter(!is.na(document))
#^ Here I checked to see if I should keep the document column in. It seems like these links are invalid, so I'm going to select them out for now.
select(!document, !inspection_reason)
# group_by(inspection_reason) |>
# summarise(count=n())
#^ Here I wanted to see if there was any reason to keep the "inspection_reason" column. Unfortunately, it's all NA, so I took it out.
read_csv("data/compliance.csv") |>
clean_names() |>
#  filter(!is.na(document))
#^ Here I checked to see if I should keep the document column in. It seems like these links are invalid, so I'm going to select them out for now.
select(!document, !inspection_reason)
# group_by(inspection_reason) |>
# summarise(count=n())
#^ Here I wanted to see if there was any reason to keep the "inspection_reason" column. Unfortunately, it's all NA, so I took it out.
read_csv("data/compliance.csv") |>
clean_names() |>
#  filter(!is.na(document))
#^ Here I checked to see if I should keep the document column in. It seems like these links are invalid, so I'm going to select them out for now.
select(-document, -inspection_reason)
# group_by(inspection_reason) |>
# summarise(count=n())
#^ Here I wanted to see if there was any reason to keep the "inspection_reason" column. Unfortunately, it's all NA, so I took it out.
