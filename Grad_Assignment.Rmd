---
title: "Grad Assignment"
output: html_document
date: "2023-11-19"
author: Adriana Navarro
dataset: Maryland Department of the Environment - Water and Science Administration (WSA) Compliance
link: https://opendata.maryland.gov/Energy-and-Environment/Maryland-Department-of-the-Environment-Water-and-S/hxmu-urvx
project details: https://docs.google.com/document/d/1rvEymxmdoU72bAFqYEfobgc9is7ko2aZNxu5QZj-WKU/edit#heading=h.m8hwzhwk8ccv
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Who created and maintains the data? What state program or law is it connected to?
**Answer** Maryland Department of the Environment (MDE); owner: Sharmila; This data set is connected to MDE's Annual Enforcement and Compliance Report, required by the Environment Article ยง1-301(d), Annotated Code of Maryland.

2. What does each row represent?

**Answer** Each row represents a Water and Science Administration (WSA) Inspection

3. What is the chronological and geographic scope of this data? 

**Answer** The data set was created in 2021 but was last updated Nov. 25, 2023. As for the geographic scope, I group_by and summarised the county column and found it covers all the counties, Baltimore City County and three other categories: "Not Yet Determined", "Outside of Maryland", and "Statewide".

4. If this data contains aggregates (totals), can you find itemized data that those totals are derived from? Do the totals match your own calculations using the itemized data?

**Answer** The WSA data on its own does not contain aggregates.

**Questions for data owner**
1. Why is the inspection_reason column blank? Can I have the data?
2. Why are there reports from other states?
3. What is the 00000 zip code -- is this just blank? What is the "206 7", "20619619" and "33333-3333"? (Cleaned -- could not match 33333 with a zip code in Baltimore)

#Story Ideas
1. The MTA-Purple Line. It has the highest number of reports. This could possibly be because it stretches from Bethesda to North Carrollton, so there's a lot of ground to cover in reports. However, the majority of those reports are in noncompliance. What does this entail? Is this simply because of construction going on or do they account for that aspect in the report? How is this impacting flash flooding? 

2. The crossover into other states.

3. 


```{r}
library(tidyverse)
library(dplyr)
library(janitor)
library(stringr)
library(ggplot2)
library(ggthemes)
```


```{r}
#Here I am cleaning the data. I read it in, cleaned the names, separated the location column into three separate ones that list the city, state and zip code of the complaint, and did a lot of work on the new zip column. I then took away the document and inspection_reason columns because they were empty, but I've been trying to get ahold of the creators of the spreadsheet to see if I can get the data. I'm very interested in the inspection_reason column and was disappointed to see it was completely useless here. The document links are invalid, making that column more or less useless as well.

# I commented out the two lines of code that I used to see if there was any reason to keep the "inspection_reason" column. Unfortunately, it's all NA, so I took it out. I am trying to get a hold of the data.

compliance <- read_csv("data/compliance_dec.csv") |> 
  clean_names() |> 
  separate(city_state_zip, into = c("city", "state", "zip"), sep = ",") |>
 #group_by(inspection_reason) |> 
 #summarise(count=n()) 
  mutate(inspection_date = mdy(inspection_date),
         zip = str_replace_all(zip, c("-.*$" = "", "20619619" = "20619", "206 4" = "20674")),
         zip = str_trim(zip), 
         year = year(inspection_date),
         month = month(inspection_date, label = TRUE),
         day = day(inspection_date),
         zip = case_when(zip == "" | zip == "00000" | zip == "33333" ~ NA, .default = zip)) |> 
  select(-document, -inspection_reason)

compliance
    ```

```{r}
#Here I'm checking if these are just Maryland reports and what the zip codes look like. I used this to better clean the compliance dataframe. Cleaning checklist: change the 00000 zip codes to NA, change the zip code "20619619" to "20619" (I double checked this and it is California, Maryland), change zip code "206 4" to "20674" after grabbing the address from that entry and looking it up.

#Data smells: zip codes: "", "00000", "206 7", "20619619" "33333-3333", Why are they in Illinois?? They are in DC, GA, IL, MD, VA, WV, YY (typo?), and no state available.

#Question: Which zip code has the highest number of reports?

zip_count <- compliance |> 
  group_by(zip) |> 
  summarise(count = n()) |> 
  arrange(desc(count))

zip_count

state_zip <- compliance |> 
  filter(county == "Outside of Maryland") |> 
  group_by(zip, county, state,)

state_zip

```
```{r}
# Here I wanted to group the counties to get a count of how many there were/the geographical scope of the data. There ended up being a few extra entries that weren't Maryland counties

# Question: What county has the highest count of compliance forms? Does this match with the zip code above? St. Mary's has the highest count of compliance forms, but the zip code with the highest count is in Talbot County.
compliance_counties <- compliance |> 
  group_by(county) |> 
  summarise(count = n()) |> 
  arrange(desc(count))

compliance_counties
```

```{r}

compliance_actions <- compliance |> 
  group_by(recommended_actions) |> 
  summarise(count =n()) |> 
  arrange(desc(count))

compliance_actions
```

```{r}
compliance_site_condition <- compliance |> 
  group_by(site_condition) |> 
  summarise(count =n()) |> 
  arrange(desc(count))

compliance_site_condition
```


```{r}
#What are the action required reports? A residence Hall at Bowie State University has the most. Would probably need to FOIA the records or just ask since they seemed pretty willing to help out when I called.

action_required <- compliance |> 
  filter(site_condition == "Corrective Actions Required") |> 
  group_by(site_name) |> 
 summarise(count = n()) |> 
 arrange(desc(count))


action_required
```

```{r}
#A recycling company (World Recycling Company, 11 counts of significant noncompliance) received the most "Significant Noncompliance" counts. Now let's see how they pan out combined with Noncompliance. The recycling company is still pretty far down when combined.

sig_non <- compliance |> 
  filter(site_status == "Significant Noncompliance") |> 
  group_by(site_name) |> 
  summarise(count = n()) |> 
  arrange(desc(count))

sig_non



sig_norm_noncom <- compliance |> 
  filter(site_status == "Significant Noncompliance"  | site_status == "Noncompliance") |> 
  group_by(site_name) |> 
  summarise(count = n()) |> 
  arrange(desc(count))


sig_norm_noncom
```



```{r}
#Wowowowowow the Purple Line has over 500 total reports, making it the site with the highest count of reports. This makes sense seeing how it's a large project that spans across different zip codes.
names <- compliance |> 
#  filter(site_name == "MTA-Purple Line") |> 
  group_by(site_name) |> 
  summarise(count =n()) |> 
  arrange(desc(count))

names
```

```{r}
# Does the MTA-Purple Line have the highest count of noncompliance as well? Yep. Note: It had over 260 counts when I used the site_status column instead. I'm using the site_condition column for now since it seems like originally this is the correct column. The "site_status" column should say whether the case is "active" or "closed."

noncompliance_condition <- compliance |> 
  filter(site_condition == "Noncompliance") |> 
  group_by(site_name) |> 
  summarise(count = n()) |> 
  arrange(desc(count))

noncompliance_condition


noncompliance_status <- compliance |> 
  filter(site_status == "Noncompliance") |> 
  group_by(site_name) |> 
  summarise(count = n()) |> 
  arrange(desc(count))

noncompliance_status
```

```{r}
#I just wanted to make this dataframe for future coding
purple_line <- compliance |> 
  filter(site_name == "MTA-Purple Line")
purple_line
```

```{r}
#
purple_line_types <- purple_line |> 
  filter(site_status == "Noncompliance") |> 
  group_by(inspection_type) |> 
  summarise(count =n()) |> 
  mutate(percent = count/(sum(count))*100) |> 
  arrange(desc(count))

purple_line_types

write_csv(purple_line_types, "data/purple_line_types.csv")
#Exporting this for a datawrapper map. See below. 
```

```{r}
#I'm more comfortable with using datawrapper, but I wanted to try to make this a bar chart in ggplot. Here's the datawrapper chart:

#And here's the ggplot chart: 

purple_line_types |> 
  ggplot() +
  geom_bar(aes(x =reorder(inspection_type, percent), weight = percent), fill = "lightblue", color = "lightblue") +
  coord_flip() + 
  theme_economist() +
labs(
  title="Majority of Purple Line's noncompliance results were storm water management",
    x = "noncompliance types",
    y = "percent",
    caption = "source: Maryland Department of the Environment"
) +
  theme(
    plot.title = element_text(size = 12, margin = margin(b = 15), hjust = 1.3), 
    axis.title.y = element_text(margin = margin(r = 10)),  
    axis.title.x = element_text(margin = margin(t = 10)),  
    plot.margin = margin(t = 30, r = 10, b = 10, l = 10)  
  )

```


```{r}
#Construction on the Purple Line actually paused around the pandemic and restarted during the summer of 2022. What was going on during that period of limbo? Why were there still compliance reports/was it not taken care of during that time? How did this impact the surrounding area/environment? This can be especially concerning since storm water management is the top inspection type for this site. Did this impact flash flood risk?
noncompliance_per_year <- compliance |> 
  filter(site_status == "Noncompliance") |> 
  group_by(year) |> 
  summarise(count = n()) |> 
  arrange(year)
  
  noncompliance_per_year
```

```{r} 
#Was there a year with more noncompliance issues than others? Yes, 2023. Construction began Aug. 2017, paused Sept. 2020 and restarted summer of 2022
purple_line_time <- purple_line |> 
  group_by(year) |> 
  summarise(count = n()) |> 
  arrange(year)
  
purple_line_time
```

#Okay, time to start exploring another data set. I want to see if I can merge this with the census. I want to see the population in each county. Maybe I can break it up by race/ethnicity for a good comparison. 

```{r}
mdzip <- get_acs(geography="zcta", variables = "B01003_001", year=2021) |> 
  clean_names() |> 
  rename(zip = geoid, total_pop = estimate) |>
  select(-name, -variable, -moe)

mdzip

md_white <- get_acs(geography="zcta", variables = "B02001_002", year=2021) |> 
  clean_names() |> 
  rename(zip = geoid, white_pop = estimate) |>
  select(-name, -variable, -moe)

md_black <- get_acs(geography="zcta", variables = "B02001_003", year=2021) |> 
  clean_names() |> 
  rename(zip = geoid, black_pop = estimate) |>
  select(-name, -variable, -moe)

md_indigenous <- get_acs(geography="zcta", variables = "B02001_004", year=2021)|> 
  clean_names() |> 
  rename(zip = geoid, indigenous_pop = estimate) |>
  select(-name, -variable, -moe)

md_asian <- get_acs(geography="zcta", variables = "B02001_005", year=2021)|> 
  clean_names() |> 
  rename(zip = geoid, asian_pop = estimate) |>
  select(-name, -variable, -moe)

md_hawaii_pi <- get_acs(geography="zcta", variables = "B02001_006", year=2021)|> 
  clean_names() |> 
  rename(zip = geoid, hawaii_pi_pop = estimate) |>
  select(-name, -variable, -moe)

md_other <- get_acs(geography="zcta", variables = "B02001_007", year=2021)|>
  clean_names() |> 
  rename(zip = geoid, other_pop = estimate) |>
  select(-name, -variable, -moe)

md_two_more <- get_acs(geography="zcta", variables = "B02001_008", year=2021)|> 
  clean_names() |> 
  rename(zip = geoid, two_more_pop = estimate) |>
  select(-name, -variable, -moe)

md_hispanic <- get_acs(geography="zcta", variables = "B03001_003", year=2021)|> 
  clean_names() |> 
  rename(zip = geoid, hispanic_pop = estimate) |>
  select(-name, -variable, -moe)

```

# Joining

```{r}
#Let's start with the census data. The number of people by race/ethnicity were a little meaningless just by looking at them, so I made columns that showed their percentage of the population instead. Since "Hispanic" is not a race, it shouldn't be counted as its own. There will be overlap.

census_data <- mdzip |> 
  left_join(md_white, by = "zip") |> 
  left_join(md_black, by = "zip") |> 
  left_join(md_indigenous, by = "zip") |>
  left_join(md_asian, by = "zip") |> 
  left_join(md_hawaii_pi, by = "zip") |> 
  left_join(md_other, by = "zip") |> 
  left_join(md_two_more, by = "zip") |>
  left_join(md_hispanic, by = "zip") |> 
  mutate(perc_white = (white_pop/total_pop)*100) |> 
  mutate(perc_black = (black_pop/total_pop)*100) |>
  mutate(perc_indigenous = (indigenous_pop/total_pop)*100) |> 
  mutate(perc_asian = (asian_pop/total_pop)*100) |>  
  mutate(perc_hawaii_pi = (hawaii_pi_pop/total_pop)*100) |> 
  mutate(perc_other = (other_pop/total_pop)*100) |>   
  mutate(perc_two_more = (two_more_pop/total_pop)*100) |> 
  mutate(perc_hispanic = (hispanic_pop/total_pop)*100) |> 
  select(-white_pop, -black_pop, -indigenous_pop, -asian_pop, -hawaii_pi_pop, -other_pop, -two_more_pop, -hispanic_pop)

 census_data 
```

#I want to focus on noncompliance records for this. Possibly more focused on issues that were fixed/ones that haven't been resolved.

```{r}
#Let's look at just Maryland stuff right now. I'm clumping "Significant Noncompliance" together with "Noncompliance" since they're in the same vein.

md_reports_noncompliant <- compliance |> 
  filter(state == "MD", site_condition == "Significant Noncompliance"  | site_condition == "Noncompliance") |>  
  group_by(zip) |> 
  summarise(number_of_noncompliant = n()) |> 
  arrange(desc(number_of_noncompliant))

md_reports_noncompliant
```

```{r}
md_noncompliant_zip <- md_reports_noncompliant |> 
  left_join(census_data, by = "zip")

md_noncompliant_zip
```

```{r}
# Attempt at income

mdzip_income <- get_acs(geography="zcta", variables = "B19013_001", year=2021) |> 
  clean_names() |> 
  rename(zip = geoid, median_income = estimate) |>
  select(-name, -variable, -moe)

mdzip_income
```
```{r}
md_income_noncompliance <- md_reports_noncompliant |> 
  left_join(mdzip_income, by = "zip")

md_income_noncompliance

```

```{r}
ggplot(md_income_noncompliance, aes(x=median_income, y=number_of_noncompliant)) +
  geom_point()+
  labs(
    title="Majority WSA noncompliant sites were in zip codes below state's median income",
    subtitle= "Zip codes that earned below the state's median income of $108k had more noncompliant sites.",
    x = "median household income",
    y = "number of noncompliant results",
    caption = "source: Maryland Department of the Environment, U.S. Census") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    geom_text(check_overlap = TRUE, size = 2, aes(label = zip), hjust = 0.5, vjust = -0.5)
```

















```{r}
#Attempt at heat map

#Tidying up the data for the plotting
map_data <- md_noncompliant_zip |> 
  gather(key = "race", value = "count", -zip, -number_of_noncompliant)
```

```{r}

heatmap_plot <- ggplot(map_data, aes(x = zip, y = race, fill = count)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(title = "Reports by Race Demographics in Each Zip Code",
       x = "Zip Code",
       y = "Race",
       fill = "Number of Reports")

heatmap_plot
```




























#Now let's make a dataframe of just the corrective_actions_required reports in MD. How are they spread out across the state?

compliance_corrective_required <- md_reports |>
  filter(site_condition == "Corrective Actions Required")

required_zip <- compliance_corrective_required |> 
  group_by(zip) |> 
  summarise(number_of_corrective_actions_required = n()) |> 
  arrange(desc(number_of_corrective_actions_required)) |> 
  mutate(total = sum(number_of_corrective_actions_required),
         percent_of_corrections = (number_of_corrective_actions_required/total)*100) |> 
  select(-total)

required_zip
```


```{r}
demographics_near_required_actions <- required_zip |> 
  left_join(census_data, by = "zip") |> 
  arrange(desc(percent_of_corrections))

demographics_near_required_actions
```

```{r}
md_zip_map <- md_reports_zip |> 
  left_join(census_data, by = "zip") |> 
  mutate(reports_per_capita = (number_of_reports/total_pop)*10,000) |> 
  select(zip, reports_per_capita)
  
md_zip_map

write_csv(md_zip_map, "data/md_map.csv")
```




